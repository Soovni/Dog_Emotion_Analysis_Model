{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 학습 데이터 만들기\n",
        "1. 우리가 학습한 모델을 사용하여 120개 품종에 대해 1차 분류<br>\n",
        "  -> 각 품종에 대해 happy와 angry로 분류\n",
        "2. 우리가 분류된 이미지를 다시 한번 정제\n",
        "  - happy로 분류된 이미지 중에 happy와 거리가 먼 이미지 제거\n",
        "  - angry로 분류된 이미지 중에 happy인 이미지 제거\n",
        "  - 이미지를 일일이 볼 필요 없이 스르륵 보면서 맞지 않은 사진만 제거해주세요"
      ],
      "metadata": {
        "id": "EmJgED-MMDZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 구글 코랩에 파일 업로드\n",
        "-> 왼쪽 파일 부분에 마우스 우클릭 해서 업로드<br><br>\n",
        "업로드 해야 할 파일 목록\n",
        "- model_weights.pth (학습된 모델의 가중치)\n",
        "- 각자 image.zip파일 업로드"
      ],
      "metadata": {
        "id": "Quf-vTJxbD3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 압축 파일 해제\n",
        "-> 오른쪽 파일 부분에 압축파일이 업로드되는 중에는 파란 동그라미가 표시돼요. 그거 다 끝난 다음에 아래 코드 실행해주세요"
      ],
      "metadata": {
        "id": "UaKhPCCHpVAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_file = zipfile.ZipFile('image1.zip') # 각자 zip파일 맞게 image2.zip / image3.zip / ... 이렇게 수정해주세요\n",
        "zip_file.extractall('image')"
      ],
      "metadata": {
        "id": "r810MFBapXJ7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 이미지 파일의 이름들 저장한 리스트"
      ],
      "metadata": {
        "id": "2E2IGWn2pfjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "img_list=sorted(os.listdir('image'))"
      ],
      "metadata": {
        "id": "eApGBFYvqWSv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xC_28QwVsv8C",
        "outputId": "3eda8840-ca8b-41c2-9620-d483d49564b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Afghan_hound',\n",
              " 'African_hunting_dog',\n",
              " 'Airedale',\n",
              " 'American_Staffordshire_terrier',\n",
              " 'Appenzeller',\n",
              " 'Australian_terrier',\n",
              " 'Bedlington_terrier',\n",
              " 'Bernese_mountain_dog',\n",
              " 'Blenheim_spaniel',\n",
              " 'Border_collie',\n",
              " 'Border_terrier',\n",
              " 'Boston_bull',\n",
              " 'Bouvier_des_Flandres',\n",
              " 'Brabancon_griffon',\n",
              " 'affenpinscher',\n",
              " 'basenji',\n",
              " 'basset',\n",
              " 'beagle',\n",
              " 'black-and-tan_coonhound',\n",
              " 'bloodhound',\n",
              " 'bluetick',\n",
              " 'borzoi',\n",
              " 'boxer',\n",
              " 'briard']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습된 모델 사용하는 방법\n",
        "- 학습된 가중치를 불러온다\n",
        "- 학습할 때와 동일한 구조로 모델을 설정한다\n",
        "- 가중치를 load하여 모델을 정의한다"
      ],
      "metadata": {
        "id": "UcMn3atxbvTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# 위에 설정했던대로 ResNet18 동일하게 정의\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "num_classes = 2  # happy와 angry\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# 가중치 불러오기\n",
        "model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
        "model.eval()  # 평가 모드"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_2fqaIp5bCHe",
        "outputId": "5e8c9ac6-078c-4c0f-ff5f-8c913377a9ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 143MB/s]\n",
            "<ipython-input-4-705de637875b>:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"model_weights.pth\"))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 사용해서 이미지 happy, angry로 분류\n",
        "분류된 결과는 result 폴더 안에 저장된다"
      ],
      "metadata": {
        "id": "8oPfCc0CBbtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from torchvision.models import resnet18\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def classify_image(model, image_path):\n",
        "    try:\n",
        "        image = Image.open(image_path).convert(\"RGB\")  # 이미지 열기 및 RGB 변환\n",
        "        input_tensor = transform(image).unsqueeze(0).to(device)  # 배치 차원 추가 및 GPU로 이동\n",
        "        output = model(input_tensor)\n",
        "        _, predicted = torch.max(output, 1)  # 예측 클래스\n",
        "        return predicted.item()  # 0: happy, 1: angry\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "input_folder = \"image\"  # 상위 폴더\n",
        "output_folder = \"result\"  # 결과 저장 폴더\n",
        "os.makedirs(output_folder, exist_ok=True)  # 결과 폴더 생성\n",
        "\n",
        "for folder_name in img_list:\n",
        "    folder_path = os.path.join(input_folder, folder_name)\n",
        "    if not os.path.isdir(folder_path):  # 폴더인지 확인\n",
        "        continue\n",
        "\n",
        "    # 결과 폴더 하위에 해당 폴더 생성\n",
        "    happy_folder = os.path.join(output_folder, folder_name, \"happy\")\n",
        "    angry_folder = os.path.join(output_folder, folder_name, \"not_happy\")\n",
        "    os.makedirs(happy_folder, exist_ok=True)\n",
        "    os.makedirs(angry_folder, exist_ok=True)\n",
        "\n",
        "    # 해당 폴더의 이미지 분류\n",
        "    for filename in os.listdir(folder_path):\n",
        "        image_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        # 이미지 파일만 처리\n",
        "        if not filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
        "            continue\n",
        "\n",
        "        # 분류 수행\n",
        "        result = classify_image(model, image_path)\n",
        "\n",
        "        if result is not None:\n",
        "            if result == 0:  # happy\n",
        "                shutil.copy(image_path, os.path.join(happy_folder, filename))\n",
        "            elif result == 1:  # angry\n",
        "                shutil.copy(image_path, os.path.join(angry_folder, filename))\n",
        "\n",
        "        #print(f\"{folder_name}/{filename} -> {'happy' if result == 0 else 'not happy' if result == 1 else 'error'}\")\n",
        "\n",
        "print(\"모든 이미지 분류 완료!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvB5DLh9b9-D",
        "outputId": "e6af6129-8cb4-48ff-bf37-260a31f89243"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모든 이미지 분류 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 이미지 분류 결과 압축파일로 다운로드"
      ],
      "metadata": {
        "id": "e09HmCuYBWv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# 1. result 폴더를 ZIP 파일로 압축\n",
        "shutil.make_archive('result', 'zip', 'result')  # 'result.zip' 생성\n",
        "\n",
        "# 2. 로컬 저장소로 다운로드\n",
        "files.download('result.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0sJJ6pHtvxq4",
        "outputId": "d6a330c3-40e5-4c04-d83b-a61a9e1d93d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f1063875-b9ff-451b-a20f-388c7a0c3f63\", \"result.zip\", 195083923)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "91-zeaIF2CHE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}